{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98a9e1e-0ad6-40df-9072-e25838423e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/allennlp/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from allennlp.data import Token, Vocabulary, TokenIndexer, Tokenizer\n",
    "from allennlp.data.fields import ListField, TextField\n",
    "from allennlp.data.token_indexers import (\n",
    "    SingleIdTokenIndexer,\n",
    "    TokenCharactersIndexer,\n",
    "    ELMoTokenCharactersIndexer,\n",
    "    PretrainedTransformerIndexer,\n",
    "    PretrainedTransformerMismatchedIndexer,\n",
    ")\n",
    "from allennlp.data.tokenizers import (\n",
    "    CharacterTokenizer,\n",
    "    PretrainedTransformerTokenizer,\n",
    "    SpacyTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    ")\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    TokenCharactersEncoder,\n",
    "    ElmoTokenEmbedder,\n",
    "    PretrainedTransformerEmbedder,\n",
    "    PretrainedTransformerMismatchedEmbedder,\n",
    ")\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673b93f1-b863-4dd4-845b-71d50fe6d572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo tokens: [This, is, some, text, .]\n",
      "ELMo tensors: {'elmo_tokens': {'elmo_tokens': tensor([[259,  85, 105, 106, 116, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261],\n",
      "        [259, 106, 116, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261],\n",
      "        [259, 116, 112, 110, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261],\n",
      "        [259, 117, 102, 121, 117, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261],\n",
      "        [259,  47, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261]])}}\n"
     ]
    }
   ],
   "source": [
    "tokenizer: Tokenizer = WhitespaceTokenizer()\n",
    "token_indexer: TokenIndexer = ELMoTokenCharactersIndexer()\n",
    "vocab = Vocabulary()\n",
    "text = \"This is some text .\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"ELMo tokens:\", tokens)\n",
    "text_field = TextField(tokens, {\"elmo_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "padding_lengths = text_field.get_padding_lengths()\n",
    "\n",
    "tensor_dict = text_field.as_tensor(padding_lengths)\n",
    "print(\"ELMo tensors:\", tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba21460-3dc1-4425-b204-6d49e3efa29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29.0/29.0 [00:00<00:00, 50.1kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 696kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 208k/208k [00:01<00:00, 142kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 426k/426k [00:04<00:00, 89.6kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT tokens: [[CLS], Some, text, with, an, extra, ##ord, ##ina, ##rily, long, id, ##ent, ##ifier, ., [SEP]]\n",
      "BERT tensors: {'bert_tokens': {'token_ids': tensor([  101,  1789,  3087,  1114,  1126,  3908,  6944,  2983, 11486,  1263,\n",
      "        25021,  3452, 17792,   119,   102]), 'mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True]), 'type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}}\n"
     ]
    }
   ],
   "source": [
    "transformer_model = \"bert-base-cased\"\n",
    "tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)\n",
    "token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)\n",
    "text = \"Some text with an extraordinarily long identifier.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"BERT tokens:\", tokens)\n",
    "text_field = TextField(tokens, {\"bert_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "\n",
    "tensor_dict = text_field.as_tensor(text_field.get_padding_lengths())\n",
    "print(\"BERT tensors:\", tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c345735f-8c6d-438a-8482-d3cde959026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens: [This, context, is, f, ##rand, ##ib, ##ulous, .]\n",
      "Question tokens: [What, is, the, context, like, ?]\n",
      "Combined tokens: [[CLS], This, context, is, f, ##rand, ##ib, ##ulous, ., [SEP], What, is, the, context, like, ?, [SEP]]\n",
      "Combined BERT tensors: {'bert_tokens': {'token_ids': tensor([  101,  1188,  5618,  1110,   175, 13141, 13292, 14762,   119,   102,\n",
      "         1327,  1110,  1103,  5618,  1176,   136,   102]), 'mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True]), 'type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])}}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PretrainedTransformerTokenizer(\n",
    "    model_name=transformer_model,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "\n",
    "context_text = \"This context is frandibulous.\"\n",
    "question_text = \"What is the context like?\"\n",
    "context_tokens = tokenizer.tokenize(context_text)\n",
    "question_tokens = tokenizer.tokenize(question_text)\n",
    "print(\"Context tokens:\", context_tokens)\n",
    "print(\"Question tokens:\", question_tokens)\n",
    "\n",
    "combined_tokens = tokenizer.add_special_tokens(context_tokens, question_tokens)\n",
    "print(\"Combined tokens:\", combined_tokens)\n",
    "\n",
    "text_field = TextField(combined_tokens, {\"bert_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "\n",
    "tensor_dict = text_field.as_tensor(text_field.get_padding_lengths())\n",
    "print(\"Combined BERT tensors:\", tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4d9fa-b4c4-42d3-939b-2cbc0ee51093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits text into words (instead of wordpieces or characters).  For ELMo, you can\n",
    "# just use any word-level tokenizer that you like, though for best results you\n",
    "# should use the same tokenizer that was used with ELMo, which is an older version\n",
    "# of spacy.  We're using a whitespace tokenizer here for ease of demonstration\n",
    "# with binder.\n",
    "tokenizer: Tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# Represents each token with an array of characters in a way that ELMo expects.\n",
    "token_indexer: TokenIndexer = ELMoTokenCharactersIndexer()\n",
    "\n",
    "# Both ELMo and BERT do their own thing with vocabularies, so we don't need to add\n",
    "# anything, but we do need to construct the vocab object so we can use it below.\n",
    "# (And if you have any labels in your data that need indexing, you'll still need\n",
    "# this.)\n",
    "vocab = Vocabulary()\n",
    "\n",
    "text = \"This is some text .\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"ELMo tokens:\", tokens)\n",
    "\n",
    "text_field = TextField(tokens, {\"elmo_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "\n",
    "# We typically batch things together when making tensors, which requires some\n",
    "# padding computation.  Don't worry too much about the padding for now.\n",
    "padding_lengths = text_field.get_padding_lengths()\n",
    "\n",
    "tensor_dict = text_field.as_tensor(padding_lengths)\n",
    "print(\"ELMo tensors:\", tensor_dict)\n",
    "\n",
    "# Any transformer model name that huggingface's transformers library supports will\n",
    "# work here.  Under the hood, we're grabbing pieces from huggingface for this\n",
    "# part.\n",
    "transformer_model = \"bert-base-cased\"\n",
    "\n",
    "# To do modeling with BERT correctly, we can't use just any tokenizer; we need to\n",
    "# use BERT's tokenizer.\n",
    "tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)\n",
    "\n",
    "# Represents each wordpiece with an id from BERT's vocabulary.\n",
    "token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)\n",
    "\n",
    "text = \"Some text with an extraordinarily long identifier.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"BERT tokens:\", tokens)\n",
    "\n",
    "text_field = TextField(tokens, {\"bert_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "\n",
    "tensor_dict = text_field.as_tensor(text_field.get_padding_lengths())\n",
    "print(\"BERT tensors:\", tensor_dict)\n",
    "\n",
    "# Now we'll do an example with paired text, to show the right way to handle [SEP]\n",
    "# tokens in AllenNLP.  We have built-in ways of handling this for two text pieces.\n",
    "# If you have more than two text pieces, you'll have to manually add the special\n",
    "# tokens.  The way we're doing this requires that you use a\n",
    "# PretrainedTransformerTokenizer, not the abstract Tokenizer class.\n",
    "\n",
    "# Splits text into wordpieces, but without adding special tokens.\n",
    "tokenizer = PretrainedTransformerTokenizer(\n",
    "    model_name=transformer_model,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "\n",
    "context_text = \"This context is frandibulous.\"\n",
    "question_text = \"What is the context like?\"\n",
    "context_tokens = tokenizer.tokenize(context_text)\n",
    "question_tokens = tokenizer.tokenize(question_text)\n",
    "print(\"Context tokens:\", context_tokens)\n",
    "print(\"Question tokens:\", question_tokens)\n",
    "\n",
    "combined_tokens = tokenizer.add_special_tokens(context_tokens, question_tokens)\n",
    "print(\"Combined tokens:\", combined_tokens)\n",
    "\n",
    "text_field = TextField(combined_tokens, {\"bert_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "\n",
    "tensor_dict = text_field.as_tensor(text_field.get_padding_lengths())\n",
    "print(\"Combined BERT tensors:\", tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8abee07e-c0ab-45cf-821d-d0a98f3f464a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo tokens: [This, is, some, text.]\n",
      "ELMo tensors: {'elmo_tokens': {'elmo_tokens': tensor([[259,  85, 105, 106, 116, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261],\n",
      "        [259, 106, 116, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261],\n",
      "        [259, 116, 112, 110, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261],\n",
      "        [259, 117, 102, 121, 117,  47, 260, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "         261, 261, 261, 261, 261, 261, 261, 261]])}}\n"
     ]
    }
   ],
   "source": [
    "tokenizer: Tokenizer = WhitespaceTokenizer()\n",
    "token_indexer: TokenIndexer = ELMoTokenCharactersIndexer()\n",
    "vocab = Vocabulary()\n",
    "text = \"This is some text.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"ELMo tokens:\", tokens)\n",
    "text_field = TextField(tokens, {\"elmo_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "token_tensor = text_field.as_tensor(text_field.get_padding_lengths())\n",
    "print(\"ELMo tensors:\", token_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3f92770-1ec4-4f05-acd6-9ba087751344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo embedded tokens: tensor([[[ 0.7915,  0.0000, -0.2392, -0.0000, -0.8512, -0.0000, -0.4313,\n",
      "          -1.4576,  0.5932, -0.5559,  0.0271,  0.0000, -0.0000,  0.7962,\n",
      "          -0.4822, -0.0000, -0.0000,  0.9739, -0.0000, -0.0000, -1.5000,\n",
      "          -1.4474, -0.9234, -0.0000,  0.0000, -0.1619, -0.2561, -0.0000,\n",
      "          -0.0000,  0.0000, -0.0000, -0.3873],\n",
      "         [ 0.8214,  0.0000, -0.4139, -0.8952, -0.8725,  0.3791, -0.0000,\n",
      "          -0.0000,  0.6740, -0.8773,  0.0000, -0.1073, -0.4150,  0.2156,\n",
      "           0.0000,  0.3758,  0.0000,  0.5719, -1.3625, -0.6818, -0.0000,\n",
      "          -0.0000, -1.4208,  0.3838,  0.0000, -0.5148, -0.7528,  0.0000,\n",
      "          -0.0000, -0.0000, -0.4018,  0.6609],\n",
      "         [ 0.7935,  1.2918, -0.0000, -0.0000, -0.5219, -0.2793, -0.8381,\n",
      "          -0.0000,  0.0000, -0.6763,  0.0150,  0.2454, -0.0000,  0.9078,\n",
      "          -0.1125,  0.4098, -0.0000,  0.0487, -0.0000,  0.0000, -0.6707,\n",
      "          -0.0000, -0.0000,  0.2709,  1.7239, -0.6564,  0.0000,  0.0000,\n",
      "          -0.0000,  0.0000,  0.0000,  0.2791],\n",
      "         [-4.2713, -0.0000, -0.0000,  0.0000, -0.0000,  2.5919, -4.8008,\n",
      "           4.8264, -0.0000, -0.0000,  2.1167,  1.9269,  0.0000, -0.0000,\n",
      "           2.8465,  3.7315, -0.0000,  0.0000, -3.3836, -0.0000, -0.0000,\n",
      "           1.8097, -0.0000,  2.9327,  0.0000, -0.0000, -1.6098,  2.8753,\n",
      "           1.4234,  0.0000, -2.1357, -0.0000]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We're using a tiny, toy version of ELMo to demonstrate this.\n",
    "elmo_options_file = (\n",
    "    \"https://allennlp.s3.amazonaws.com/models/elmo/test_fixture/options.json\"\n",
    ")\n",
    "elmo_weight_file = (\n",
    "    \"https://allennlp.s3.amazonaws.com/models/elmo/test_fixture/lm_weights.hdf5\"\n",
    ")\n",
    "elmo_embedding = ElmoTokenEmbedder(\n",
    "    options_file=elmo_options_file, weight_file=elmo_weight_file\n",
    ")\n",
    "\n",
    "embedder = BasicTextFieldEmbedder(token_embedders={\"elmo_tokens\": elmo_embedding})\n",
    "\n",
    "tensor_dict = text_field.batch_tensors([token_tensor])\n",
    "embedded_tokens = embedder(tensor_dict)\n",
    "print(\"ELMo embedded tokens:\", embedded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8225e529-e88a-46b5-bea2-75459614fae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elmo_tokens': {'elmo_tokens': tensor([[259,  85, 105, 106, 116, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261],\n",
       "          [259, 106, 116, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261],\n",
       "          [259, 116, 112, 110, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261],\n",
       "          [259, 117, 102, 121, 117,  47, 260, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "           261, 261, 261, 261, 261, 261, 261, 261]])}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5776089-12b7-4dd1-84f2-d13458e43464",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = token_tensor['elmo_tokens']['elmo_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d6f9cd8-d277-4b79-a5f7-c03b479985bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a66312d0-fbaf-48e0-8f41-a7125d5a138e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder(text_field.batch_tensors([token_tensor])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cded41b-cd8b-4f3d-b2fe-08ecc34edb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_dict['elmo_tokens']['elmo_tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df3d19a-7887-4999-b191-39df22ecebe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eff4d5-087e-4aa5-9221-fa761d2e79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's easiest to get ELMo input by just running the data code.  See the\n",
    "# exercise above for an explanation of this code.\n",
    "tokenizer: Tokenizer = WhitespaceTokenizer()\n",
    "token_indexer: TokenIndexer = ELMoTokenCharactersIndexer()\n",
    "vocab = Vocabulary()\n",
    "text = \"This is some text.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"ELMo tokens:\", tokens)\n",
    "text_field = TextField(tokens, {\"elmo_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "token_tensor = text_field.as_tensor(text_field.get_padding_lengths())\n",
    "print(\"ELMo tensors:\", token_tensor)\n",
    "\n",
    "# We're using a tiny, toy version of ELMo to demonstrate this.\n",
    "elmo_options_file = (\n",
    "    \"https://allennlp.s3.amazonaws.com/models/elmo/test_fixture/options.json\"\n",
    ")\n",
    "elmo_weight_file = (\n",
    "    \"https://allennlp.s3.amazonaws.com/models/elmo/test_fixture/lm_weights.hdf5\"\n",
    ")\n",
    "elmo_embedding = ElmoTokenEmbedder(\n",
    "    options_file=elmo_options_file, weight_file=elmo_weight_file\n",
    ")\n",
    "\n",
    "embedder = BasicTextFieldEmbedder(token_embedders={\"elmo_tokens\": elmo_embedding})\n",
    "\n",
    "tensor_dict = text_field.batch_tensors([token_tensor])\n",
    "embedded_tokens = embedder(tensor_dict)\n",
    "print(\"ELMo embedded tokens:\", embedded_tokens)\n",
    "\n",
    "\n",
    "# Again, it's easier to just run the data code to get the right output.\n",
    "\n",
    "# We're using the smallest transformer model we can here, so that it runs on\n",
    "# binder.\n",
    "transformer_model = \"google/reformer-crime-and-punishment\"\n",
    "tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)\n",
    "token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)\n",
    "text = \"Some text with an extraordinarily long identifier.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Transformer tokens:\", tokens)\n",
    "text_field = TextField(tokens, {\"bert_tokens\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "token_tensor = text_field.as_tensor(text_field.get_padding_lengths())\n",
    "print(\"Transformer tensors:\", token_tensor)\n",
    "\n",
    "embedding = PretrainedTransformerEmbedder(model_name=transformer_model)\n",
    "\n",
    "embedder = BasicTextFieldEmbedder(token_embedders={\"bert_tokens\": embedding})\n",
    "\n",
    "tensor_dict = text_field.batch_tensors([token_tensor])\n",
    "embedded_tokens = embedder(tensor_dict)\n",
    "print(\"Transformer embedded tokens:\", embedded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b7d89-6626-4be1-ae5b-0e6e52b7dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import Vocabulary\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "import torch\n",
    "\n",
    "# This is what gets created by TextField.as_tensor with a SingleIdTokenIndexer;\n",
    "# see the exercises above.\n",
    "token_tensor = {\"tokens\": {\"tokens\": torch.LongTensor([1, 3, 2, 1, 4, 3])}}\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.add_tokens_to_namespace(\n",
    "    [\"This\", \"is\", \"some\", \"text\", \".\"], namespace=\"token_vocab\"\n",
    ")\n",
    "\n",
    "glove_file = \"https://allennlp.s3.amazonaws.com/datasets/glove/glove.6B.50d.txt.gz\"\n",
    "\n",
    "# This is for embedding each token.\n",
    "embedding = Embedding(\n",
    "    vocab=vocab,\n",
    "    vocab_namespace=\"token_vocab\",\n",
    "    embedding_dim=50,\n",
    "    pretrained_file=glove_file,\n",
    ")\n",
    "\n",
    "embedder = BasicTextFieldEmbedder(token_embedders={\"tokens\": embedding})\n",
    "\n",
    "embedded_tokens = embedder(token_tensor)\n",
    "print(embedded_tokens.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
